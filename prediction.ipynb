{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following on from the exploration done in the exploratory_analysis.ipynb notebook, I will work on the actual prediction task for the ionosphere dataset from UCI to be found here: https://archive.ics.uci.edu/ml/datasets/Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "rnd.seed(111)\n",
    "\n",
    "data = pd.read_csv('ionosphere_processed.csv', index_col=0)\n",
    "data.drop('1', axis=1, inplace=True)\n",
    "X = data.iloc[:, :-1].as_matrix()\n",
    "y = np.array(data.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ionosphere dataset is a classification problem with the target variable taking on either 'good' or 'bad' values, I will start with a logistic regression and then go from there trying to improve on that performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_classifier = LogisticRegression(solver='liblinear')\n",
    "logit_classifier.fit(X, y)\n",
    "yhat = logit_classifier.predict(X)\n",
    "probabilities = logit_classifier.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'yhat' now contains the model's predictions for each observation while 'probabilities' gives each observation's probability to belong to either class. Since this is a two-class classification problem, the probabilities should in all cases sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision made by the classifier is then simply to predict that an observation belongs to the class with the higher probability. In this case, that makes sense; however, there might also be other contexts in which one would like to classify a given observation as belonging to some class already if it has a probability for that class greater than e.g. a threshold of 30%. The reason might be that the costs for a false negative significantly outweigh the costs of a false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how well the classifier did in predicting the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97,  29],\n",
       "       [  5, 220]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat_0 = confusion_matrix(y_true=y, y_pred=yhat)\n",
    "conf_mat_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-left of the confusion matrix are true positives, top-right are false negatives, bottom-left are false positives, and bottom-right are true negatives. It seems the simple logistic regression classifier is not doing too badly; however, these performance metrics are giving too positive an impression. The reason for that is that I used the entire dataset to train the classifier and subsequently evaluated over the entire dataset, too. This can easily lead to an underestimation of prediction errors and give too 'rosy' a picture. \n",
    "\n",
    "In order to avoid this, one could for example split the dataset into two pieces using the first to train the classifier and the second to evaluate it. This can give a better impression of how well the classifier is able to generalize, i.e. predict correct labels for previously-unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 33)\n",
      "(116, 33)\n",
      "(235,)\n",
      "(116,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split() function has helpfully divided the dataset into altogether four pieces: two for the features, and two for the target variable. The samples were randomly selected.\n",
    "\n",
    "Now I can train the classifier only on the train subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit_classifier = LogisticRegression(solver='liblinear')\n",
    "logit_classifier.fit(X_train, y_train)\n",
    "yhat = logit_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 15],\n",
       "       [ 0, 67]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat_1 = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
    "conf_mat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the confusion matrix from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90313390313390318"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conf_mat_0[0, 0] + conf_mat_0[1, 1]) / sum(sum(conf_mat_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87068965517241381"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conf_mat_1[0, 0] + conf_mat_1[1, 1]) / sum(sum(conf_mat_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the fraction of correctly classified observations has gone down, demonstrating that using the entire dataset for training and testing underestimates the classification error for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the idea of using subsets of the dataset for training and other subsets for testing one step further leads to what is called k-fold cross-validation (https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k-fold cross-validation, the dataset is divided into k equally sized parts. The classifier is then trained on k-1 of those and evaluated on the remaining part. This process is repeated k times so that each partition (or fold) is used as a test set once. The prediction error can then be averaged over all k folds in order to better estimate how the classifier would generalize. Let's try this out before moving on from logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n=X.shape[0], n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_conf_mat = []\n",
    "for train_idx, test_idx in kf:\n",
    "    classifier = LogisticRegression(solver='liblinear')\n",
    "    classifier.fit(X[train_idx, :], y[train_idx])\n",
    "    yhat = classifier.predict(X[test_idx, :])\n",
    "    list_conf_mat.append(confusion_matrix(y_true=y[test_idx],\n",
    "                                          y_pred=yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  8]\n",
      " [ 1 49]]\n",
      "\n",
      "[[21 10]\n",
      " [ 3 36]]\n",
      "\n",
      "[[20  8]\n",
      " [ 0 42]]\n",
      "\n",
      "[[18  6]\n",
      " [ 1 45]]\n",
      "\n",
      "[[15  7]\n",
      " [ 0 48]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for conf_mat in list_conf_mat: \n",
    "    print(conf_mat)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87464788732394361"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(cm[0, 0] + cm[1, 1]) / sum(sum(cm)) for cm in list_conf_mat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the logistic regression correctly classifies about 86% of the observations as evaluated by 5-fold cross-validation. From here on out, I will try to improve on this baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a decision tree next, first without any cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126   0]\n",
      " [  0 225]]\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X, y)\n",
    "yhat = tree.predict(X)\n",
    "conf_mat = confusion_matrix(y_true=y, y_pred=yhat)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, the decision tree correctly classifies 100% of the samples! Now, as mentioned before, this success rate may be the result of not using any cross validation. So let's try again, this time with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  9]\n",
      " [ 4 63]]\n"
     ]
    }
   ],
   "source": [
    "tree.fit(X_train, y_train)\n",
    "yhat = tree.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887931034483\n"
     ]
    }
   ],
   "source": [
    "print((conf_mat[0, 0] + conf_mat[1, 1]) / sum(sum(conf_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, the performance is reduced, as expected. It will likely get worse with k-fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n=X.shape[0], n_folds=5, shuffle=True)\n",
    "list_conf_mat = []\n",
    "for train_idx, test_idx in kf:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X[train_idx, :], y[train_idx])\n",
    "    yhat = tree.predict(X[test_idx, :])\n",
    "    list_conf_mat.append(confusion_matrix(y_true=y[test_idx], y_pred=yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how the classifier now did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85762575452716305"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(cm[0, 0] + cm[1, 1]) / sum(sum(cm)) for cm in list_conf_mat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually did worse than the logistic regression. Next, I'll try with a random forest."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
